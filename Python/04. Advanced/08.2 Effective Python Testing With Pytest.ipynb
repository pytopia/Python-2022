{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344ffcd5",
   "metadata": {},
   "source": [
    "<img src=\"../../images/banners/python-advanced.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51fe0ae",
   "metadata": {},
   "source": [
    "# <img src=\"../../images/logos/python.png\" width=\"23\"/> Effective Python Testing With Pytest \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faafd13",
   "metadata": {},
   "source": [
    "## <img src=\"../../images/logos/toc.png\" width=\"20\"/> Table of Contents \n",
    "* [How to Install `pytest`](#how_to_install_`pytest`)\n",
    "* [What Makes `pytest` So Useful?](#what_makes_`pytest`_so_useful?)\n",
    "    * [Less Boilerplate](#less_boilerplate)\n",
    "    * [State and Dependency Management](#state_and_dependency_management)\n",
    "    * [Test Filtering](#test_filtering)\n",
    "    * [Test Parametrization](#test_parametrization)\n",
    "    * [Plugin-Based Architecture](#plugin-based_architecture)\n",
    "* [Fixtures: Managing State and Dependencies](#fixtures:_managing_state_and_dependencies)\n",
    "    * [When to Create Fixtures](#when_to_create_fixtures)\n",
    "    * [When to Avoid Fixtures](#when_to_avoid_fixtures)\n",
    "    * [Fixtures at Scale](#fixtures_at_scale)\n",
    "* [Marks: Categorizing Tests](#marks:_categorizing_tests)\n",
    "* [Parametrization: Combining Tests](#parametrization:_combining_tests)\n",
    "* [Durations Reports: Fighting Slow Tests](#durations_reports:_fighting_slow_tests)\n",
    "* [Useful `pytest` Plugins](#useful_`pytest`_plugins)\n",
    "    * [`pytest-randomly`](#`pytest-randomly`)\n",
    "    * [`pytest-cov`](#`pytest-cov`)\n",
    "    * [`pytest-django`](#`pytest-django`)\n",
    "    * [`pytest-bdd`](#`pytest-bdd`)\n",
    "* [Conclusion](#conclusion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a979052",
   "metadata": {},
   "source": [
    "[Testing your code](https://realpython.com/python-testing/) brings a wide variety of benefits. It increases your confidence that the code behaves as you expect and ensures that changes to your code won’t cause regressions. Writing and maintaining tests is hard work, so you should leverage all the tools at your disposal to make it as painless as possible. [`pytest`](https://docs.pytest.org/) is one of the best tools you can use to boost your testing productivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd192549",
   "metadata": {},
   "source": [
    "**In this tutorial, you’ll learn:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9cb1f",
   "metadata": {},
   "source": [
    "- What **benefits** `pytest` offers\n",
    "- How to ensure your tests are **stateless**\n",
    "- How to make repetitious tests more **comprehensible**\n",
    "- How to run **subsets** of tests by name or custom groups\n",
    "- How to create and maintain **reusable** testing utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699d75d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"how_to_install_`pytest`\"></a>\n",
    "\n",
    "## How to Install `pytest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba5343",
   "metadata": {},
   "source": [
    "To follow along with some of the examples in this tutorial, you’ll need to install `pytest`. As with most [Python packages](https://realpython.com/python-modules-packages/), you can install `pytest` in a [virtual environment](https://realpython.com/python-virtual-environments-a-primer/) from [PyPI](https://realpython.com/pypi-publish-python-package/) using [`pip`](https://realpython.com/what-is-pip/):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a0dcb",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python -m pip install pytest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0f18f",
   "metadata": {},
   "source": [
    "The `pytest` command will now be available in your installation environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d8a28",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"what_makes_`pytest`_so_useful?\"></a>\n",
    "\n",
    "## What Makes `pytest` So Useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e90a116",
   "metadata": {},
   "source": [
    "If you’ve written unit tests for your Python code before, then you may have used Python’s built-in **`unittest`** module. `unittest` provides a solid base on which to build your test suite, but it has a few shortcomings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca42678a",
   "metadata": {},
   "source": [
    "A number of third-party testing frameworks attempt to address some of the issues with `unittest`, and [`pytest` has proven to be one of the most popular](https://realpython.com/courses/test-driven-development-pytest/). `pytest` is a feature-rich, plugin-based ecosystem for testing your Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f6d94b",
   "metadata": {},
   "source": [
    "If you haven’t had the pleasure of using `pytest` yet, then you’re in for a treat! Its philosophy and features will make your testing experience more productive and enjoyable. With `pytest`, common tasks require less code and advanced tasks can be achieved through a variety of time-saving commands and plugins. It will even run your existing tests out of the box, including those written with `unittest`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296cbbcb",
   "metadata": {},
   "source": [
    "As with most frameworks, some development patterns that make sense when you first start using `pytest` can start causing pains as your test suite grows. This tutorial will help you understand some of the tools `pytest` provides to keep your testing efficient and effective even as it scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf5200",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"less_boilerplate\"></a>\n",
    "\n",
    "### Less Boilerplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6e4d3",
   "metadata": {},
   "source": [
    "Most functional tests follow the Arrange-Act-Assert model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c2df35",
   "metadata": {},
   "source": [
    "1. **Arrange**, or set up, the conditions for the test\n",
    "2. **Act** by calling some function or method\n",
    "3. **Assert** that some end condition is true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55b466",
   "metadata": {},
   "source": [
    "Testing frameworks typically hook into your test’s [assertions](https://realpython.com/lessons/assertions-and-tryexcept/) so that they can provide information when an assertion fails. `unittest`, for example, provides a number of helpful assertion utilities out of the box. However, even a small set of tests requires a fair amount of [boilerplate code](https://en.wikipedia.org/wiki/Boilerplate_code)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83c8d1",
   "metadata": {},
   "source": [
    "Imagine you’d like to write a test suite just to make sure `unittest` is working properly in your project. You might want to write one test that always passes and one that always fails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b33df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from unittest import TestCase\n",
    "\n",
    "class TryTesting(TestCase):\n",
    "    def test_always_passes(self):\n",
    "        self.assertTrue(True)\n",
    "\n",
    "    def test_always_fails(self):\n",
    "        self.assertTrue(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ecb337",
   "metadata": {},
   "source": [
    "You can then run those tests from the command line using the `discover` option of `unittest`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64115cd",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ python -m unittest discover\n",
    "F.\n",
    "============================================================\n",
    "FAIL: test_always_fails (test_with_unittest.TryTesting)\n",
    "------------------------------------------------------------\n",
    "Traceback (most recent call last):\n",
    "  File \"/.../test_with_unittest.py\", line 9, in test_always_fails\n",
    "    self.assertTrue(False)\n",
    "AssertionError: False is not True\n",
    "\n",
    "------------------------------------------------------------\n",
    "Ran 2 tests in 0.001s\n",
    "\n",
    "FAILED (failures=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0217bf80",
   "metadata": {},
   "source": [
    "As expected, one test passed and one failed. You’ve proven that `unittest` is working, but look at what you had to do:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f35279",
   "metadata": {},
   "source": [
    "1. Import the `TestCase` class from `unittest`\n",
    "2. Create `TryTesting`, a [subclass](https://realpython.com/python3-object-oriented-programming/) of `TestCase`\n",
    "3. Write a method in `TryTesting` for each test\n",
    "4. Use one of the `self.assert*` methods from `unittest.TestCase` to make assertions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3caeade",
   "metadata": {},
   "source": [
    "That’s a significant amount of code to write, and because it’s the minimum you need for *any* test, you’d end up writing the same code over and over. `pytest` simplifies this workflow by allowing you to use Python’s `assert` keyword directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e0da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_with_pytest.py\n",
    "\n",
    "def test_always_passes():\n",
    "    assert True\n",
    "\n",
    "def test_always_fails():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36445a85",
   "metadata": {},
   "source": [
    "That’s it. You don’t have to deal with any imports or classes. Because you can use the `assert` keyword, you don’t need to learn or remember all the different `self.assert*` methods in `unittest`, either. If you can write an expression that you expect to evaluate to `True`, then `pytest` will test it for you. You can run it using the `pytest` command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac6af6",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ pytest\n",
    "================== test session starts =============================\n",
    "platform darwin -- Python 3.7.3, pytest-5.3.0, py-1.8.0, pluggy-0.13.0\n",
    "rootdir: /.../effective-python-testing-with-pytest\n",
    "collected 2 items\n",
    "\n",
    "test_with_pytest.py .F                                          [100%]\n",
    "\n",
    "======================== FAILURES ==================================\n",
    "___________________ test_always_fails ______________________________\n",
    "\n",
    "    def test_always_fails():\n",
    ">       assert False\n",
    "E       assert False\n",
    "\n",
    "test_with_pytest.py:5: AssertionError\n",
    "============== 1 failed, 1 passed in 0.07s =========================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd89a5e6",
   "metadata": {},
   "source": [
    "`pytest` presents the test results differently than `unittest`. The report shows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbbe22a",
   "metadata": {},
   "source": [
    "1. The system state, including which versions of Python, `pytest`, and any plugins you have installed\n",
    "2. The `rootdir`, or the directory to search under for configuration and tests\n",
    "3. The number of tests the runner discovered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad978f",
   "metadata": {},
   "source": [
    "The output then indicates the status of each test using a syntax similar to `unittest`: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72272e87",
   "metadata": {},
   "source": [
    "- **A dot (`.`)** means that the test passed. \n",
    "- **`F`** means that the test has failed. \n",
    "- **`E`** means that the test raised an unexpected exception. For pytest, any uncaught exception thrown in a test function is a failure, including but not limited to assertion errors. So error is reserved for a failure in a fixture.\n",
    "\n",
    "Output may also include `s`, `x`, and `X`, which Will be explained later in pytest marks. In short:\n",
    "- **`s`** means skipped.\n",
    "- **`x`** means xfailed.\n",
    "- **`X`** means xpassed.\n",
    "\n",
    "\n",
    "For example:\n",
    "```python\n",
    "@pytest.fixture\n",
    "def error_fixture():\n",
    "    assert False\n",
    "def test_fail(error_fixture):\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788fcaac",
   "metadata": {},
   "source": [
    "For tests that fail, the report gives a detailed breakdown of the failure. In the example above, the test failed because `assert False` always fails. Finally, the report gives an overall status report of the test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62765818",
   "metadata": {},
   "source": [
    "Here are a few more quick assertion examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1fb4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_uppercase():\n",
    "    assert \"loud noises\".upper() == \"LOUD NOISES\"\n",
    "\n",
    "def test_reversed():\n",
    "    assert list(reversed([1, 2, 3, 4])) == [4, 3, 2, 1]\n",
    "\n",
    "def test_some_primes():\n",
    "    assert 37 in {\n",
    "        num\n",
    "        for num in range(1, 50)\n",
    "        if num != 1 and not any([num % div == 0 for div in range(2, num)])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a278c7",
   "metadata": {},
   "source": [
    "The learning curve for `pytest` is shallower than it is for `unittest` because you don’t need to learn new constructs for most tests. Also, the use of `assert`, which you may have used before in your implementation code, makes your tests more understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7749b88c",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"state_and_dependency_management\"></a>\n",
    "\n",
    "### State and Dependency Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c06dc",
   "metadata": {},
   "source": [
    "Your tests will often depend on pieces of data or [test doubles](https://en.wikipedia.org/wiki/Test_double) for some of the objects in your code. In `unittest`, you might extract these dependencies into `setUp()` and `tearDown()` methods so each test in the class can make use of them. But in doing so, you may inadvertently make the test’s dependence on a particular piece of data or object entirely **implicit**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e502ec4",
   "metadata": {},
   "source": [
    "Over time, implicit dependencies can lead to a complex tangle of code that you have to unwind to make sense of your tests. Tests should help you make your code more understandable. If the tests themselves are difficult to understand, then you may be in trouble!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f33a42",
   "metadata": {},
   "source": [
    "`pytest` takes a different approach. It leads you toward **explicit** dependency declarations that are still reusable thanks to the availability of [fixtures](https://docs.pytest.org/en/latest/fixture.html). `pytest` fixtures are functions that create data or test doubles or initialize some system state for the test suite. Any test that wants to use a fixture must explicitly accept it as an argument, so dependencies are always stated up front."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c8d01",
   "metadata": {},
   "source": [
    "Fixtures can also make use of other fixtures, again by declaring them explicitly as dependencies. That means that, over time, your fixtures can become bulky and modular. Although the ability to insert fixtures into other fixtures provides enormous flexibility, it can also make managing dependencies more challenging as your test suite grows. Later in this tutorial, you’ll learn [more about fixtures](#fixtures:_managing_state_and_dependencies) and try a few techniques for handling these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b878002",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"test_filtering\"></a>\n",
    "\n",
    "### Test Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1db61",
   "metadata": {},
   "source": [
    "As your test suite grows, you may find that you want to run just a few tests on a feature and save the full suite for later. `pytest` provides a few ways of doing this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a2429",
   "metadata": {},
   "source": [
    "- **Name-based filtering**: You can limit `pytest` to running only those tests whose fully qualified names match a particular expression. You can do this with the `-k` parameter.\n",
    "- **Directory scoping**: By default, `pytest` will run only those tests that are in or under the current directory.\n",
    "- **Test categorization**: `pytest` can include or exclude tests from particular categories that you define. You can do this with the `-m` parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfcd89",
   "metadata": {},
   "source": [
    "Test categorization in particular is a subtly powerful tool. `pytest` enables you to create **marks**, or custom labels, for any test you like. A test may have multiple labels, and you can use them for granular control over which tests to run. Later in this tutorial, you’ll see an example of [how `pytest` marks work](#marks-categorizing-tests) and learn how to make use of them in a large test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb05a57",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"test_parametrization\"></a>\n",
    "\n",
    "### Test Parametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7062bb21",
   "metadata": {},
   "source": [
    "When you’re testing functions that process data or perform generic transformations, you’ll find yourself writing many similar tests. They may differ only in the [input or output](https://realpython.com/python-input-output/) of the code being tested. This requires duplicating test code, and doing so can sometimes obscure the behavior you’re trying to test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a15419",
   "metadata": {},
   "source": [
    "`unittest` offers a way of collecting several tests into one, but they don’t show up as individual tests in result reports. If one test fails and the rest pass, then the entire group will still return a single failing result. `pytest` offers its own solution in which each test can pass or fail independently. You’ll see [how to parametrize tests](#parametrization-combining-tests) with `pytest` later in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca121d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"plugin-based_architecture\"></a>\n",
    "\n",
    "### Plugin-Based Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57449302",
   "metadata": {},
   "source": [
    "One of the most beautiful features of `pytest` is its openness to customization and new features. Almost every piece of the program can be cracked open and changed. As a result, `pytest` users have developed a rich ecosystem of helpful plugins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332dce8",
   "metadata": {},
   "source": [
    "Although some `pytest` plugins focus on specific frameworks like [Django](https://www.djangoproject.com/), others are applicable to most test suites. You’ll see [details on some specific plugins](#useful-pytest-plugins) later in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf420d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"fixtures:_managing_state_and_dependencies\"></a>\n",
    "\n",
    "## Fixtures: Managing State and Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c13e1d0",
   "metadata": {},
   "source": [
    "`pytest` fixtures are a way of providing data, test doubles, or state setup to your tests. Fixtures are functions that can return a wide range of values. Each test that depends on a fixture must explicitly accept that fixture as an argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccd9ebd",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"when_to_create_fixtures\"></a>\n",
    "\n",
    "### When to Create Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1c869",
   "metadata": {},
   "source": [
    "Imagine you’re writing a function, `format_data_for_display()`, to process the data returned by an API endpoint. The data represents a list of people, each with a given name, family name, and job title. The function should output a list of strings that include each person’s full name (their `given_name` followed by their `family_name`), a colon, and their `title`. To test this, you might write the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_display(people):\n",
    "    ...  # Implement this!\n",
    "\n",
    "def test_format_data_for_display():\n",
    "    people = [\n",
    "        {\n",
    "            \"given_name\": \"Alfonsa\",\n",
    "            \"family_name\": \"Ruiz\",\n",
    "            \"title\": \"Senior Software Engineer\",\n",
    "        },\n",
    "        {\n",
    "            \"given_name\": \"Sayid\",\n",
    "            \"family_name\": \"Khan\",\n",
    "            \"title\": \"Project Manager\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    assert format_data_for_display(people) == [\n",
    "        \"Alfonsa Ruiz: Senior Software Engineer\",\n",
    "        \"Sayid Khan: Project Manager\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e1455",
   "metadata": {},
   "source": [
    "Now suppose you need to write another function to transform the data into comma-separated values for use in [Excel](https://realpython.com/openpyxl-excel-spreadsheets-python/). The test would look awfully similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_excel(people):\n",
    "    ... # Implement this!\n",
    "\n",
    "def test_format_data_for_excel():\n",
    "    people = [\n",
    "        {\n",
    "            \"given_name\": \"Alfonsa\",\n",
    "            \"family_name\": \"Ruiz\",\n",
    "            \"title\": \"Senior Software Engineer\",\n",
    "        },\n",
    "        {\n",
    "            \"given_name\": \"Sayid\",\n",
    "            \"family_name\": \"Khan\",\n",
    "            \"title\": \"Project Manager\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    assert format_data_for_excel(people) == \"\"\"given,family,title\n",
    "Alfonsa,Ruiz,Senior Software Engineer\n",
    "Sayid,Khan,Project Manager\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6634cf",
   "metadata": {},
   "source": [
    "If you find yourself writing several tests that all make use of the same underlying test data, then a fixture may be in your future. You can pull the repeated data into a single function decorated with `@pytest.fixture` to indicate that the function is a `pytest` fixture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def example_people_data():\n",
    "    return [\n",
    "        {\n",
    "            \"given_name\": \"Alfonsa\",\n",
    "            \"family_name\": \"Ruiz\",\n",
    "            \"title\": \"Senior Software Engineer\",\n",
    "        },\n",
    "        {\n",
    "            \"given_name\": \"Sayid\",\n",
    "            \"family_name\": \"Khan\",\n",
    "            \"title\": \"Project Manager\",\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4959c",
   "metadata": {},
   "source": [
    "You can use the fixture by adding it as an argument to your tests. Its value will be the return value of the fixture function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595efb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_format_data_for_display(example_people_data):\n",
    "    assert format_data_for_display(example_people_data) == [\n",
    "        \"Alfonsa Ruiz: Senior Software Engineer\",\n",
    "        \"Sayid Khan: Project Manager\",\n",
    "    ]\n",
    "\n",
    "def test_format_data_for_excel(example_people_data):\n",
    "    assert format_data_for_excel(example_people_data) == \"\"\"given,family,title\n",
    "Alfonsa,Ruiz,Senior Software Engineer\n",
    "Sayid,Khan,Project Manager\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c46ce9e",
   "metadata": {},
   "source": [
    "Each test is now notably shorter but still has a clear path back to the data it depends on. Be sure to name your fixture something specific. That way, you can quickly determine if you want to use it when writing new tests in the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3364876d",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"when_to_avoid_fixtures\"></a>\n",
    "\n",
    "### When to Avoid Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da2bcf9",
   "metadata": {},
   "source": [
    "Fixtures are great for extracting data or objects that you use across multiple tests. They aren’t always as good for tests that require slight variations in the data. Littering your test suite with fixtures is no better than littering it with plain data or objects. It might even be worse because of the added layer of indirection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5738c8",
   "metadata": {},
   "source": [
    "As with most abstractions, it takes some practice and thought to find the right level of fixture use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80eab4",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"fixtures_at_scale\"></a>\n",
    "\n",
    "### Fixtures at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26851ee3",
   "metadata": {},
   "source": [
    "As you extract more fixtures from your tests, you might see that some fixtures could benefit from further extraction. Fixtures are **modular**, so they can depend on other fixtures. You may find that fixtures in two separate test modules share a common dependency. What can you do in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1378fc",
   "metadata": {},
   "source": [
    "You can move fixtures from test [modules](https://realpython.com/python-modules-packages/) into more general fixture-related modules. That way, you can import them back into any test modules that need them. This is a good approach when you find yourself using a fixture repeatedly throughout your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589b8b2",
   "metadata": {},
   "source": [
    "`pytest` looks for `conftest.py` modules throughout the directory structure. Each `conftest.py` provides configuration for the file tree `pytest` finds it in. You can use any fixtures that are defined in a particular `conftest.py` throughout the file’s parent directory and in any subdirectories. This is a great place to put your most widely used fixtures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec757311",
   "metadata": {},
   "source": [
    "Another interesting use case for fixtures is in guarding access to resources. Imagine that you’ve written a test suite for code that deals with [API calls](https://realpython.com/api-integration-in-python/). You want to ensure that the test suite doesn’t make any real network calls, even if a test accidentally executes the real network call code. `pytest` provides a [`monkeypatch`](https://docs.pytest.org/en/latest/monkeypatch.html) fixture to replace values and behaviors, which you can use to great effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd579844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conftest.py\n",
    "\n",
    "import pytest\n",
    "import requests\n",
    "\n",
    "@pytest.fixture(autouse=True)\n",
    "def disable_network_calls(monkeypatch):\n",
    "    def stunted_get():\n",
    "        raise RuntimeError(\"Network access not allowed during testing!\")\n",
    "    monkeypatch.setattr(requests, \"get\", lambda *args, **kwargs: stunted_get())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa3a45b",
   "metadata": {},
   "source": [
    "By placing `disable_network_calls()` in `conftest.py` and adding the `autouse=True` option, you ensure that network calls will be disabled in every test across the suite. Any test that executes code calling `requests.get()` will raise a `RuntimeError` indicating that an unexpected network call would have occurred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ec8c93",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"marks:_categorizing_tests\"></a>\n",
    "\n",
    "## Marks: Categorizing Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81cfff3",
   "metadata": {},
   "source": [
    "In any large test suite, some of the tests will inevitably be slow. They might test timeout behavior, for example, or they might exercise a broad area of the code. Whatever the reason, it would be nice to avoid running *all* the slow tests when you’re trying to iterate quickly on a new feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24a55a",
   "metadata": {},
   "source": [
    "`pytest` enables you to define categories for your tests and provides options for including or excluding categories when you run your suite. You can mark a test with any number of categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03068cc",
   "metadata": {},
   "source": [
    "Marking tests is useful for categorizing tests by subsystem or dependencies. If some of your tests require access to a database, for example, then you could create a `@pytest.mark.database_access` mark for them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebc619c",
   "metadata": {},
   "source": [
    "When the time comes to run your tests, you can still run them all by default with the `pytest` command. If you’d like to run only those tests that require database access, then you can use `pytest -m database_access`. To run all tests *except* those that require database access, you can use `pytest -m \"not database_access\"`. You can even use an `autouse` fixture to limit database access to those tests marked with `database_access`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c715e78",
   "metadata": {},
   "source": [
    "Some plugins expand on the functionality of marks by guarding access to resources. The [`pytest-django`](https://pytest-django.readthedocs.io/en/latest/) plugin provides a `django_db` mark. Any tests without this mark that try to access the database will fail. The first test that tries to access the database will trigger the creation of Django’s test database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14675ee7",
   "metadata": {},
   "source": [
    "The requirement that you add the `django_db` mark nudges you toward stating your dependencies explicitly. That’s the `pytest` philosophy, after all! It also means that you can run tests that don’t rely on the database much more quickly, because `pytest -m \"not django_db\"` will prevent the test from triggering database creation. The time savings really add up, especially if you’re diligent about running your tests frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156bd81",
   "metadata": {},
   "source": [
    "`pytest` provides a few marks out of the box:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f3780",
   "metadata": {},
   "source": [
    "- **`skip`** skips a test unconditionally.\n",
    "- **`skipif`** skips a test if the expression passed to it evaluates to `True`.\n",
    "- **`xfail`** indicates that a test is expected to fail, so if the test *does* fail, the overall suite can still result in a passing status.\n",
    "- **`parametrize`** (note the spelling) creates multiple variants of a test with different values as arguments. You’ll learn more about this mark shortly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546a4ddf-2267-4cee-b2b9-8024cbfaa707",
   "metadata": {},
   "source": [
    "**`xfail`** tests as implemented in pytest are distinct from tests that are expected to produce an error. The xfail test should pass and is expected to pass in the future, but is expected to fail given the current state of the software. In contrast, tests that are expected to produce an error fail if there is no such error. Such error tests are e.g. useful to assert that input validation succeeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e33856b",
   "metadata": {},
   "source": [
    "You can see a list of all the marks `pytest` knows about by running `pytest --markers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1032d60a-074d-48d9-bf55-d54e47a4f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db03021-1482-41eb-bfe7-40adc29ae30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'darwin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8ff31",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"parametrization:_combining_tests\"></a>\n",
    "\n",
    "## Parametrization: Combining Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16dd93b",
   "metadata": {},
   "source": [
    "You saw earlier in this tutorial how `pytest` fixtures can be used to reduce code duplication by extracting common dependencies. Fixtures aren’t quite as useful when you have several tests with slightly different inputs and expected outputs. In these cases, you can [**parametrize**](http://doc.pytest.org/en/latest/example/parametrize.html) a single test definition, and `pytest` will create variants of the test for you with the parameters you specify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089486d",
   "metadata": {},
   "source": [
    "Imagine you’ve written a function to tell if a string is a [palindrome](https://en.wikipedia.org/wiki/Palindrome). An initial set of tests could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc11e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_is_palindrome_empty_string():\n",
    "    assert is_palindrome(\"\")\n",
    "\n",
    "def test_is_palindrome_single_character():\n",
    "    assert is_palindrome(\"a\")\n",
    "\n",
    "def test_is_palindrome_mixed_casing():\n",
    "    assert is_palindrome(\"Bob\")\n",
    "\n",
    "def test_is_palindrome_with_spaces():\n",
    "    assert is_palindrome(\"Never odd or even\")\n",
    "\n",
    "def test_is_palindrome_with_punctuation():\n",
    "    assert is_palindrome(\"Do geese see God?\")\n",
    "\n",
    "def test_is_palindrome_not_palindrome():\n",
    "    assert not is_palindrome(\"abc\")\n",
    "\n",
    "def test_is_palindrome_not_quite():\n",
    "    assert not is_palindrome(\"abab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149c8bd",
   "metadata": {},
   "source": [
    "All of these tests except the last two have the same shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_is_palindrome_<in some situation>():\n",
    "    assert is_palindrome(\"<some string>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b6793e",
   "metadata": {},
   "source": [
    "You can use `@pytest.mark.parametrize()` to fill in this shape with different values, reducing your test code significantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379afe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"palindrome\", [\n",
    "    \"\",\n",
    "    \"a\",\n",
    "    \"Bob\",\n",
    "    \"Never odd or even\",\n",
    "    \"Do geese see God?\",\n",
    "])\n",
    "def test_is_palindrome(palindrome):\n",
    "    assert is_palindrome(palindrome)\n",
    "\n",
    "@pytest.mark.parametrize(\"non_palindrome\", [\n",
    "    \"abc\",\n",
    "    \"abab\",\n",
    "])\n",
    "def test_is_palindrome_not_palindrome(non_palindrome):\n",
    "    assert not is_palindrome(non_palindrome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76528ea4",
   "metadata": {},
   "source": [
    "The first argument to `parametrize()` is a comma-delimited string of parameter names. The second argument is a [list](https://realpython.com/courses/lists-tuples-python/) of either [tuples](https://realpython.com/python-lists-tuples/) or single values that represent the parameter value(s). You could take your parametrization a step further to combine all your tests into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa5f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"maybe_palindrome, expected_result\", [\n",
    "    (\"\", True),\n",
    "    (\"a\", True),\n",
    "    (\"Bob\", True),\n",
    "    (\"Never odd or even\", True),\n",
    "    (\"Do geese see God?\", True),\n",
    "    (\"abc\", False),\n",
    "    (\"abab\", False),\n",
    "])\n",
    "def test_is_palindrome(maybe_palindrome, expected_result):\n",
    "    assert is_palindrome(maybe_palindrome) == expected_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3364bdc7",
   "metadata": {},
   "source": [
    "Even though this shortened your code, it’s important to note that in this case, it didn’t do much to clarify your test code. Use parametrization to separate the test data from the test behavior so that it’s clear what the test is testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dacacf-8aad-4cc9-b825-3053ca9db284",
   "metadata": {},
   "source": [
    "## Working with custom markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088b70d-af56-4e96-927e-f4963f3cef14",
   "metadata": {},
   "source": [
    "Marking test functions and selecting them for a run\n",
    "You can “mark” a test function with custom metadata like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc789f3-3222-4027-8fff-0c9e6b4349c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.webtest\n",
    "def test_send_http():\n",
    "    pass  # perform some webtest test for your app\n",
    "\n",
    "\n",
    "def test_something_quick():\n",
    "    pass\n",
    "\n",
    "\n",
    "def test_another():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542774d-c3af-4eba-b5ab-736ee8716abe",
   "metadata": {},
   "source": [
    "You can then restrict a test run to only run tests marked with webtest:\n",
    "\n",
    "```bash\n",
    "pytest -v -m webtest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3beb73-759c-4b97-89be-bd50067d102d",
   "metadata": {},
   "source": [
    "Or the inverse, running all tests except the webtest ones:\n",
    "\n",
    "```bash\n",
    "pytest -v -m \"not webtest\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7dba75-b67a-4bab-8cb9-e43e01a7f08c",
   "metadata": {},
   "source": [
    "You can use the `-k` command line option to specify an expression which implements a substring match on the test names instead of the exact match on markers that `-m` provides. This makes it easy to select tests based on their names:\n",
    "\n",
    "The expression matching is now case-insensitive.\n",
    "\n",
    "```bash\n",
    "pytest -v -k http  # running with the above defined example module\n",
    "```\n",
    "\n",
    "And you can also run all tests except the ones that match the keyword:\n",
    "\n",
    "```bash\n",
    "pytest -k \"not send_http\" -v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0786a2-148a-4afe-aa90-a46834f5b903",
   "metadata": {},
   "source": [
    "Or to select “http” and “quick” tests:\n",
    "\n",
    "```bash\n",
    "pytest -k \"http or quick\" -v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d705fd-a6be-4bed-a104-8b1110cdf93a",
   "metadata": {},
   "source": [
    "You can use and, or, not and parentheses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8520dd38-64c7-4a58-9a26-f50944e2d54d",
   "metadata": {},
   "source": [
    "### Registering markers\n",
    "Registering markers for your test suite is simple:\n",
    "\n",
    "```yaml\n",
    "# content of pytest.ini\n",
    "[pytest]\n",
    "markers =\n",
    "    webtest: mark a test as a webtest.\n",
    "    slow: mark test as slow.\n",
    "```\n",
    "Multiple custom markers can be registered, by defining each one in its own line, as shown in above example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392e0e6",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"durations_reports:_fighting_slow_tests\"></a>\n",
    "\n",
    "## Durations Reports: Fighting Slow Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1574590",
   "metadata": {},
   "source": [
    "Each time you switch contexts from implementation code to test code, you incur some [overhead](https://en.wikipedia.org/wiki/Overhead_(computing)). If your tests are slow to begin with, then overhead can cause friction and frustration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf3c100",
   "metadata": {},
   "source": [
    "You read earlier about using marks to filter out slow tests when you run your suite. If you want to improve the speed of your tests, then it’s useful to know which tests might offer the biggest improvements. `pytest` can automatically record test durations for you and report the top offenders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c543129",
   "metadata": {},
   "source": [
    "Use the `--durations` option to the `pytest` command to include a duration report in your test results. `--durations` expects an integer value `n` and will report the slowest `n` number of tests. The output will follow your test results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c102c4",
   "metadata": {},
   "source": [
    "```sh\n",
    "$ pytest --durations=3\n",
    "3.03s call     test_code.py::test_request_read_timeout\n",
    "1.07s call     test_code.py::test_request_connection_timeout\n",
    "0.57s call     test_code.py::test_database_read\n",
    "======================== 7 passed in 10.06s ==============================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4ee1d",
   "metadata": {},
   "source": [
    "Each test that shows up in the durations report is a good candidate to speed up because it takes an above-average amount of the total testing time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b49e8dd",
   "metadata": {},
   "source": [
    "Be aware that some tests may have an invisible setup overhead. You read earlier about how the first test marked with `django_db` will trigger the creation of the Django test database. The `durations` report reflects the time it takes to set up the database in the test that triggered the database creation, which can be misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb11325c",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"useful_`pytest`_plugins\"></a>\n",
    "\n",
    "## Useful `pytest` Plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e7cfc",
   "metadata": {},
   "source": [
    "You learned about a few valuable `pytest` plugins earlier in this tutorial. You can explore those and a few others in more depth below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c5660",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"`pytest-randomly`\"></a>\n",
    "\n",
    "### `pytest-randomly`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6b597",
   "metadata": {},
   "source": [
    "[`pytest-randomly`](https://github.com/pytest-dev/pytest-randomly) does something seemingly simple but with valuable effect: It forces your tests to run in a random order. `pytest` always collects all the tests it can find before running them, so `pytest-randomly` shuffles that list of tests just before execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a082308",
   "metadata": {},
   "source": [
    "This is a great way to uncover tests that depend on running in a specific order, which means they have a **stateful dependency** on some other test. If you built your test suite from scratch in `pytest`, then this isn’t very likely. It’s more likely to happen in test suites that you migrate to `pytest`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6e92e-88f8-4ad8-8f95-9e74cdc787ff",
   "metadata": {},
   "source": [
    "Pytest will automatically find the plugin and use it when you run pytest. The output will start with an extra line that tells you the random seed that is being used:\n",
    "\n",
    "```bash\n",
    "$ pytest\n",
    "...\n",
    "platform darwin -- Python 3.7.2, pytest-4.3.1, py-1.8.0, pluggy-0.9.0\n",
    "Using --randomly-seed=1553614239\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd66df",
   "metadata": {},
   "source": [
    "The plugin will print a seed value in the configuration description. You can use that value to run the tests in the same order as you try to fix the issue.\n",
    "\n",
    "```bash\n",
    "pytest --randomly-seed=1234\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d9b48-9e7b-459c-8464-d882cbf3aa8c",
   "metadata": {},
   "source": [
    "Or more conveniently, use the special value last:\n",
    "\n",
    "```bash\n",
    "pytest --randomly-seed=last\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f559dd-7e89-4350-a9f8-294466076635",
   "metadata": {},
   "source": [
    "The plugin appears to Pytest with the name ‘randomly’. To disable it altogether, you can use the `-p` argument, for example:\n",
    "\n",
    "```bash\n",
    "pytest -p no:randomly\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c207654-f480-4b31-8b4d-01a8a9a186b7",
   "metadata": {},
   "source": [
    "Generally, `-p name` loads given plugin module name. To avoid loading of plugins, you should use the `no:` prefix, e.g. `no:doctest`, `no:randomly`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c1079",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"`pytest-cov`\"></a>\n",
    "\n",
    "### `pytest-cov`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86429a",
   "metadata": {},
   "source": [
    "If you measure how well your tests cover your implementation code, you likely use the [coverage](https://coverage.readthedocs.io/) package. [`pytest-cov`](https://pytest-cov.readthedocs.io/en/latest/) integrates coverage, so you can run `pytest --cov` to see the test coverage report.\n",
    "\n",
    "```bash\n",
    "pytest --cov=myproj tests/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f6f66",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"`pytest-django`\"></a>\n",
    "\n",
    "### `pytest-django`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e087683",
   "metadata": {},
   "source": [
    "[`pytest-django`](https://pytest-django.readthedocs.io/en/latest/) provides a handful of useful fixtures and marks for dealing with Django tests. You saw the `django_db` mark earlier in this tutorial, and the `rf` fixture provides direct access to an instance of Django’s [`RequestFactory`](https://docs.djangoproject.com/en/3.0/topics/testing/advanced/#django.test.RequestFactory). The `settings` fixture provides a quick way to set or override Django settings. This is a great boost to your Django testing productivity! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c45c9",
   "metadata": {},
   "source": [
    "If you’re interested in learning more about using `pytest` with Django, then check out [How to Provide Test Fixtures for Django Models in Pytest](https://realpython.com/django-pytest-fixtures/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228fc78",
   "metadata": {},
   "source": [
    "You can see which other plugins are available for `pytest` with this extensive [list of third-party plugins](http://plugincompat.herokuapp.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232f376-7a29-4f19-89dc-837f723778a0",
   "metadata": {},
   "source": [
    "## Example of All Pytest Possible Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb05bb5-cb11-46f3-91ef-8b9c6be07339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# content of test_example.py\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def error_fixture():\n",
    "    assert 0\n",
    "\n",
    "def test_ok():\n",
    "    print(\"ok\")\n",
    "\n",
    "def test_fail():\n",
    "    assert 0\n",
    "\n",
    "def test_error(error_fixture):\n",
    "    pass\n",
    "\n",
    "def test_skip():\n",
    "    pytest.skip(\"skipping this test\")\n",
    "\n",
    "def test_xfail():\n",
    "    pytest.xfail(\"xfailing this test\")\n",
    "\n",
    "@pytest.mark.xfail(reason=\"always xfail\")\n",
    "def test_xpass():\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "050a5ba5",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"conclusion\"></a>\n",
    "\n",
    "## <img src=\"../../images/logos/checkmark.png\" width=\"20\"/> Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc4143",
   "metadata": {},
   "source": [
    "`pytest` offers a core set of productivity features to filter and optimize your tests along with a flexible plugin system that extends its value even further. Whether you have a huge legacy `unittest` suite or you’re starting a new project from scratch, `pytest` has something to offer you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25817672",
   "metadata": {},
   "source": [
    "In this tutorial, you learned how to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd1b58",
   "metadata": {},
   "source": [
    "- **Fixtures** for handling test dependencies, state, and reusable functionality\n",
    "- **Marks** for categorizing tests and limiting access to external resources\n",
    "- **Parametrization** for reducing duplicated code between tests\n",
    "- **Durations** to identify your slowest tests\n",
    "- **Plugins** for integrating with other frameworks and testing tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1abea",
   "metadata": {},
   "source": [
    "Install `pytest` and give it a try. You’ll be glad you did. Happy testing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
